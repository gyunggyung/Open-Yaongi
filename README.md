# ğŸ± Open-Yaongi (ì•¼ì˜¹ì´)

**ì°¨ì„¸ëŒ€ í•œêµ­ì–´ íŠ¹í™” ì´ˆê±°ëŒ€ ì–¸ì–´ ëª¨ë¸ í”„ë¡œì íŠ¸**

Open-YaongiëŠ” NVIDIA **Nemotron-3** ì•„í‚¤í…ì²˜ì™€ Teon Optimizer, ê·¸ë¦¬ê³  **Mamba-2** (SSM) ê¸°ìˆ ì„ ê²°í•©í•˜ì—¬, H100 GPU í™˜ê²½ì—ì„œ ê·¹í•œì˜ íš¨ìœ¨ë¡œ í•™ìŠµë˜ëŠ” ì°¨ì„¸ëŒ€ AI ëª¨ë¸ì…ë‹ˆë‹¤.

## ğŸ¯ í”„ë¡œì íŠ¸ ëª©í‘œ
1.  **í•œêµ­ì–´/ìˆ˜í•™/ì½”ë”© íŠ¹í™”**: ë²”ìš©ì„±ë³´ë‹¤ëŠ” íŠ¹ì • ë„ë©”ì¸(Logic, Reasoning, Korean)ì— ê°•ì ì„ ê°€ì§„ ëª¨ë¸.
2.  **H100 íš¨ìœ¨ ê·¹ëŒ€í™”**: FP8 ê°€ì†ê³¼ Muon(Teon) ì˜µí‹°ë§ˆì´ì €ë¥¼ í†µí•´ ë‹¨ì¼ ë…¸ë“œì—ì„œë„ ìˆ˜ì‹­ì–µ íŒŒë¼ë¯¸í„° ëª¨ë¸ì„ ë¹ ë¥´ê²Œ í•™ìŠµ.
3.  **í•˜ì´ë¸Œë¦¬ë“œ ì•„í‚¤í…ì²˜**: Transformer(Attention)ì˜ ì¥ì ê³¼ Mamba(SSM)ì˜ ê¸´ ë¬¸ë§¥ ì²˜ë¦¬ ëŠ¥ë ¥ì„ ê²°í•©.

---

## ğŸ—ï¸ ëª¨ë¸ ì•„í‚¤í…ì²˜ (Yaongi-Nemotron-Teon)

ì´ ëª¨ë¸ì€ **52ê°œ ë ˆì´ì–´**ë¡œ êµ¬ì„±ëœ í•˜ì´ë¸Œë¦¬ë“œ MoE (Mixture of Experts) ëª¨ë¸ì…ë‹ˆë‹¤.

-   **Base**: Nemotron-3 Nano êµ¬ì¡° (Interleaved Attention + Mamba)
-   **Core Layers**:
    -   **Mamba-2 + MoE**: ëŒ€ë¶€ë¶„ì˜ ì—°ì‚°ì„ ë‹´ë‹¹. 64ëª…ì˜ ì „ë¬¸ê°€(Expert) ì¤‘ ìƒìœ„ 6ëª…ì„ ì„ íƒ(Top-K)í•˜ì—¬ ì²˜ë¦¬.
    -   **GQA Attention**: 6ê°œ ë ˆì´ì–´ë§ˆë‹¤ ë°°ì¹˜ë˜ì–´ ì¥ê¸° ì˜ì¡´ì„±(Long-range Dependency) ë³´ì™„.
    -   **Engram**: ë¬¸ë§¥ ì••ì¶• ë° ë‹¨ê¸°/ì¥ê¸° ê¸°ì–µ ë³´ì¡° ëª¨ë“ˆ (Layer 2, 26).
-   **Optimization**:
    -   **Teon (Muon)**: 2D í…ì„œ ì§êµí™”(Orthogonalization)ë¥¼ í†µí•œ ìˆ˜ë ´ ì†ë„ 2ë°° ê°€ì†.
    -   **Squared ReLU**: MoE ì „ë¬¸ê°€ë“¤ì˜ í™œì„±í™”ë¥¼ ë„ì™€ í•™ìŠµ ì•ˆì •ì„± í™•ë³´.
    -   **No Bias**: ëª¨ë“  ì„ í˜• ë ˆì´ì–´ì—ì„œ í¸í–¥ ì œê±°ë¡œ ì†ë„ ë° ë©”ëª¨ë¦¬ ì´ë“.

---

## ğŸ“‚ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
Open-Yaongi/
â”œâ”€â”€ experiments/
â”‚   â”œâ”€â”€ h100_moe_training/       # [ë©”ì¸] H100 í•™ìŠµ ì½”ë“œ
â”‚   â”‚   â”œâ”€â”€ train_h100_moe.py    # í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ (Teon + FP8 + MoE)
â”‚   â”‚   â”œâ”€â”€ train_tokenizer.py   # ì»¤ìŠ¤í…€ í† í¬ë‚˜ì´ì € í•™ìŠµ
â”‚   â”‚   â”œâ”€â”€ download_datasets.py # ë°ì´í„° ë‹¤ìš´ë¡œë“œ
â”‚   â”‚   â”œâ”€â”€ requirements.txt     # ì˜ì¡´ì„± ëª©ë¡
â”‚   â”‚   â””â”€â”€ README.md            # ìƒì„¸ ì‹¤í–‰ ê°€ì´ë“œ
â”‚   â””â”€â”€ colab_moe_test/          # (êµ¬) Colab í…ŒìŠ¤íŠ¸ìš© ì½”ë“œ
â”œâ”€â”€ Docs/                        # ê¸°íš ë° ì„¤ê³„ ë¬¸ì„œ
â”œâ”€â”€ run_all_vessl.sh             # Vessl AI í†µí•© ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
â””â”€â”€ README.md                    # (í˜„ì¬ íŒŒì¼)
```

---

## ğŸš€ ì‹œì‘í•˜ê¸° (Quick Start on H100)

ì´ í”„ë¡œì íŠ¸ëŠ” **Vessl AI** ë˜ëŠ” **H100 (Sm_90)** í™˜ê²½ì—ì„œ ì‹¤í–‰í•˜ë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.

### 1. í™˜ê²½ ì¤€ë¹„
- **GPU**: NVIDIA H100 (80GB) 1ì¥ ì´ìƒ
- **Docker**: `nvcr.io/nvidia/pytorch:23.10-py3` ê¸°ë°˜ (CUDA 12.2+)

### 2. í†µí•© ì‹¤í–‰ (One-Click)

í”„ë¡œì íŠ¸ ë£¨íŠ¸ì—ì„œ ì œê³µë˜ëŠ” ì‰˜ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ **ì„¤ì¹˜ -> ë°ì´í„° ì¤€ë¹„ -> í† í¬ë‚˜ì´ì € í•™ìŠµ -> ëª¨ë¸ í•™ìŠµ**ì„ í•œ ë²ˆì— ì§„í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```bash
# 1. API í‚¤ ì„¤ì • (WandB, HuggingFace)
# .env íŒŒì¼ì„ ìƒì„±í•˜ì—¬ í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”.
echo "WANDB_API_KEY=your_key" > .env
echo "HF_TOKEN=your_token" >> .env

# 2. ì‹¤í–‰ ê¶Œí•œ ë¶€ì—¬
chmod +x run_all_vessl.sh

# 3. ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹œì‘
./run_all_vessl.sh
```

ìƒì„¸í•œ ì„¤ì • ë°©ë²•ê³¼ ìˆ˜ë™ ì‹¤í–‰ ê°€ì´ë“œëŠ” [experiments/h100_moe_training/README.md](experiments/h100_moe_training/README.md)ë¥¼ ì°¸ê³ í•˜ì„¸ìš”.

---

## ğŸ“Š í•™ìŠµ ë°ì´í„° (Mix)

ì´ 15B í† í° ê·œëª¨ (ì´ˆê¸° ì‹¤í—˜)
- **í•œêµ­ì–´ (40%)**: HAERAE, KORMo, Wikipedia (ê³ í’ˆì§ˆ í…ìŠ¤íŠ¸)
- **ì˜ì–´ (20%)**: Dolma, FineWeb-Edu (ë…¼ë¦¬ ë° ì§€ì‹)
- **Code (25%)**: Stack-Edu, Dolci-Think (Reasoning & Coding)
- **Math (15%)**: FineMath (ìˆ˜ë¦¬ ë…¼ë¦¬)

---

## ğŸ“… ë¡œë“œë§µ
- [x] Colab ì†Œê·œëª¨ ê²€ì¦ ì™„ë£Œ (Nemotron-Tiny)
- [x] H100 í•™ìŠµ ì½”ë“œ ë° íŒŒì´í”„ë¼ì¸ êµ¬ì¶• (Teon Optimizer ì ìš©)
- [x] ì»¤ìŠ¤í…€ í† í¬ë‚˜ì´ì € (Vocab 32k) êµ¬í˜„
- [ ] **Phase 1**: H100 1ëŒ€ì—ì„œ 15B í† í° 1 Epoch í•™ìŠµ ì™„ë£Œ (Current)
- [ ] **Phase 2**: ëª¨ë¸ í‰ê°€ (LogicKor, KMMLU) ë° íŠœë‹
- [ ] **Phase 3**: 1T í† í° ê·œëª¨ë¡œ í™•ì¥ í•™ìŠµ

---

**Author**: gyunggyung, Gemini 3 Pro  
**License**: Apache 2.0  
