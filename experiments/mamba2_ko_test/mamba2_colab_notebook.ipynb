{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Mamba-2 Korean Nano Training (Colab T4 Fixed v2)\n",
                "\n",
                "**Update v2**: \n",
                "1. **T4 Crash Fix**: `bfloat16` 미지원 문제로 인한 `NaN` 발산을 막기 위해 `float32` 강제 변환 및 안전한 Learning Rate (0.02) 적용.\n",
                "2. **한글 깨짐 수정**: 토크나이저에 `Decoder` 설정이 빠져있어 깰져 보이던(Mojibake) 문제를 해결했습니다. 이제 `[Preview]`에서 정상적인 한글이 보입니다.\n",
                "\n",
                "이 노트북은 **Google Colab (T4 GPU)** 환경에서 Mamba-2 아키텍처 기반의 초소형(Nano) 한국어 언어 모델을 학습시키는 예제입니다."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. 필수 라이브러리 설치\n",
                "!pip install -q torch transformers datasets tokenizers"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import math\n",
                "import time\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "from torch.utils.data import DataLoader, Dataset\n",
                "from datasets import load_dataset\n",
                "from tokenizers import Tokenizer, decoders\n",
                "from tokenizers.models import BPE\n",
                "from tokenizers.trainers import BpeTrainer\n",
                "from tokenizers.pre_tokenizers import ByteLevel\n",
                "from tqdm.auto import tqdm\n",
                "\n",
                "# ==========================================\n",
                "# 0. Configuration\n",
                "# ==========================================\n",
                "class Config:\n",
                "    # Model Architecture (Nano Mamba-2)\n",
                "    vocab_size = 8000   # Reduced vocab for speed\n",
                "    d_model = 384       # Dimension\n",
                "    n_layers = 6        # Number of layers\n",
                "    n_heads = 6         # Number of heads\n",
                "    d_head = 64         # Head dimension\n",
                "    d_state = 16        # SSM state dimension\n",
                "    \n",
                "    # Training Hyperparameters\n",
                "    batch_size = 128     # Reduced for safety\n",
                "    seq_len = 256\n",
                "    lr_muon = 0.005     # Lowered significantly (0.02 -> 0.005)\n",
                "    lr_adam = 0.0005    # Lowered standard LR (0.001 -> 0.0005)\n",
                "    epochs = 3\n",
                "    \n",
                "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
                "\n",
                "config = Config()\n",
                "print(f\"Running on: {config.device}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ==========================================\n",
                "# 1. Muon Optimizer (The \"AdamW Killer\")\n",
                "# ==========================================\n",
                "def zeropower_via_newtonschulz5(G, steps=5):\n",
                "    \"\"\"\n",
                "    Muon's core: Newton-Schulz iteration for orthogonalization.\n",
                "    \"\"\"\n",
                "    assert len(G.shape) == 2\n",
                "    a, b, c = (3.4445, -4.7750,  2.0315)\n",
                "    # T4 FIX: Turing architecture does not support BFloat16 well.\n",
                "    # Force FP32 for stability.\n",
                "    X = G.float()\n",
                "    if G.size(0) > G.size(1):\n",
                "        X = X.T\n",
                "    \n",
                "    # Scale to ensure spectral norm < \\sqrt{4/3} approx\n",
                "    X = X / (X.norm() + 1e-7)\n",
                "    \n",
                "    for _ in range(steps):\n",
                "        A = X @ X.T\n",
                "        B = b * A + c * A @ A\n",
                "        X = a * X + B @ X\n",
                "    \n",
                "    if G.size(0) > G.size(1):\n",
                "        X = X.T\n",
                "    return X.to(G.dtype)\n",
                "\n",
                "class Muon(torch.optim.Optimizer):\n",
                "    \"\"\"\n",
                "    Muon Optimizer for 2D matrices (Linear layers).\n",
                "    Use AdamW for 1D tensors (Biases, LayerNorms, Embeddings).\n",
                "    \"\"\"\n",
                "    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5):\n",
                "        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)\n",
                "        super().__init__(params, defaults)\n",
                "\n",
                "    @torch.no_grad()\n",
                "    def step(self):\n",
                "        for group in self.param_groups:\n",
                "            lr = group['lr']\n",
                "            momentum = group['momentum']\n",
                "            nesterov = group['nesterov']\n",
                "            ns_steps = group['ns_steps']\n",
                "            \n",
                "            for p in group['params']:\n",
                "                if p.grad is None: continue\n",
                "                \n",
                "                g = p.grad\n",
                "                state = self.state[p]\n",
                "                \n",
                "                # Init momentum buffer\n",
                "                if 'momentum_buffer' not in state:\n",
                "                    state['momentum_buffer'] = torch.zeros_like(p)\n",
                "                \n",
                "                buf = state['momentum_buffer']\n",
                "                buf.mul_(momentum).add_(g)\n",
                "                \n",
                "                if nesterov:\n",
                "                    g = g.add(buf, alpha=momentum)\n",
                "                else:\n",
                "                    g = buf\n",
                "                \n",
                "                # Muon Update: Orthogonalize the update matrix\n",
                "                if g.ndim == 2 and g.size(0) > 32 and g.size(1) > 32:\n",
                "                    g_ortho = zeropower_via_newtonschulz5(g, steps=ns_steps)\n",
                "                    # Scale update based on dimensions to keep it consistent\n",
                "                    scale = max(1, p.size(0)/p.size(1))**0.5\n",
                "                    p.data.add_(g_ortho, alpha=-lr * scale)\n",
                "                else:\n",
                "                    p.data.add_(g, alpha=-lr)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ==========================================\n",
                "# 2. Tokenizer & Dataset\n",
                "# ==========================================\n",
                "def train_custom_tokenizer(dataset, vocab_size=8000):\n",
                "    print(f\"Training Custom BPE Tokenizer (Vocab: {vocab_size})...\")\n",
                "    \n",
                "    # Dump dataset to text file for tokenizer training\n",
                "    if not os.path.exists(\"train_corpus.txt\"):\n",
                "        with open(\"train_corpus.txt\", \"w\", encoding=\"utf-8\") as f:\n",
                "            for item in tqdm(dataset, desc=\"Exporting text\"):\n",
                "                # Combining instruction and output for causal language modeling\n",
                "                text = f\"Q:{item['instruction']} A:{item['output']}\\n\"\n",
                "                f.write(text)\n",
                "    \n",
                "    tokenizer = Tokenizer(BPE())\n",
                "    tokenizer.pre_tokenizer = ByteLevel(add_prefix_space=False)\n",
                "    # CRITICAL FIX: Add decoder to handle ByteLevel mapping (prevents Mojibake)\n",
                "    tokenizer.decoder = decoders.ByteLevel()\n",
                "    \n",
                "    trainer = BpeTrainer(\n",
                "        vocab_size=vocab_size,\n",
                "        special_tokens=[\"<pad>\", \"<s>\", \"</s>\"],\n",
                "        min_frequency=2\n",
                "    )\n",
                "    \n",
                "    tokenizer.train([\"train_corpus.txt\"], trainer)\n",
                "    return tokenizer\n",
                "\n",
                "class WrappedTokenizer:\n",
                "    def __init__(self, tokenizer):\n",
                "        self.tokenizer = tokenizer\n",
                "        self.pad_token_id = tokenizer.token_to_id(\"<pad>\")\n",
                "        self.eos_token_id = tokenizer.token_to_id(\"</s>\")\n",
                "        self.vocab_size = tokenizer.get_vocab_size()\n",
                "    \n",
                "    def encode(self, text):\n",
                "        return self.tokenizer.encode(text).ids\n",
                "    \n",
                "    def decode(self, ids, skip_special_tokens=True):\n",
                "        return self.tokenizer.decode(ids, skip_special_tokens=skip_special_tokens)\n",
                "\n",
                "class FastKoDataset(Dataset):\n",
                "    def __init__(self, dataset, tokenizer, seq_len):\n",
                "        self.samples = []\n",
                "        print(\"Tokenizing dataset...\")\n",
                "        for item in tqdm(dataset):\n",
                "            text = f\"Q:{item['instruction']} A:{item['output']}</s>\"\n",
                "            ids = tokenizer.encode(text)\n",
                "            \n",
                "            # Simple truncation/padding\n",
                "            if len(ids) > seq_len: \n",
                "                ids = ids[:seq_len]\n",
                "            else: \n",
                "                ids = ids + [tokenizer.pad_token_id]*(seq_len - len(ids))\n",
                "            \n",
                "            self.samples.append(ids)\n",
                "            \n",
                "    def __len__(self): return len(self.samples)\n",
                "    def __getitem__(self, idx): return torch.tensor(self.samples[idx], dtype=torch.long)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ==========================================\n",
                "# 3. Model Architecture (Mamba-2)\n",
                "# ==========================================\n",
                "class Mamba2Block(nn.Module):\n",
                "    def __init__(self, cfg):\n",
                "        super().__init__()\n",
                "        self.d_model = cfg.d_model\n",
                "        self.n_heads = cfg.n_heads\n",
                "        self.d_head = cfg.d_head\n",
                "        self.d_state = cfg.d_state\n",
                "        self.d_inner = cfg.n_heads * cfg.d_head\n",
                "        \n",
                "        # Combined projection for Z, X, B, C, A\n",
                "        proj_dim = (2 * self.d_inner) + (2 * self.n_heads * self.d_state) + self.n_heads\n",
                "        self.in_proj = nn.Linear(self.d_model, proj_dim, bias=False)\n",
                "        self.out_proj = nn.Linear(self.d_inner, self.d_model, bias=False)\n",
                "        self.norm = nn.LayerNorm(self.d_inner)\n",
                "        \n",
                "        self.A_log = nn.Parameter(torch.randn(self.n_heads))\n",
                "        self.D = nn.Parameter(torch.ones(self.n_heads, self.d_head))\n",
                "\n",
                "    def forward(self, x):\n",
                "        B, L, _ = x.shape\n",
                "        zxbca = self.in_proj(x)\n",
                "        \n",
                "        x_t, z_t, b_t, c_t, a_t_logits = zxbca.split([\n",
                "            self.d_inner, self.d_inner,\n",
                "            self.n_heads * self.d_state, self.n_heads * self.d_state,\n",
                "            self.n_heads\n",
                "        ], dim=-1)\n",
                "        \n",
                "        x_t = x_t.view(B, L, self.n_heads, self.d_head)\n",
                "        z_t = z_t.view(B, L, self.n_heads, self.d_head)\n",
                "        b_t = b_t.view(B, L, self.n_heads, self.d_state)\n",
                "        c_t = c_t.view(B, L, self.n_heads, self.d_state)\n",
                "        \n",
                "        decay = -torch.exp(a_t_logits.float()) \n",
                "        decay = torch.exp(decay) \n",
                "        \n",
                "        states = torch.zeros(B, self.n_heads, self.d_head, self.d_state, device=x.device)\n",
                "        y_list = []\n",
                "        \n",
                "        for t in range(L):\n",
                "            xt_step = x_t[:, t]\n",
                "            bt_step = b_t[:, t]\n",
                "            \n",
                "            decay_step = decay[:, t].view(B, self.n_heads, 1, 1)\n",
                "            states = states * decay_step\n",
                "            \n",
                "            update = torch.matmul(xt_step.unsqueeze(-1), bt_step.unsqueeze(-2))\n",
                "            states = states + update\n",
                "            \n",
                "            ct_step = c_t[:, t].unsqueeze(-1)\n",
                "            yt_step = torch.matmul(states, ct_step).squeeze(-1)\n",
                "            \n",
                "            yt_step = yt_step + xt_step * self.D.view(1, self.n_heads, self.d_head)\n",
                "            \n",
                "            y_list.append(yt_step)\n",
                "            \n",
                "        y = torch.stack(y_list, dim=1).view(B, L, self.d_inner)\n",
                "        y = y * F.silu(z_t.view(B, L, -1))\n",
                "        y = self.norm(y)\n",
                "        return self.out_proj(y)\n",
                "\n",
                "class NanoMamba2(nn.Module):\n",
                "    def __init__(self, cfg):\n",
                "        super().__init__()\n",
                "        self.embedding = nn.Embedding(cfg.vocab_size, cfg.d_model)\n",
                "        self.layers = nn.ModuleList([Mamba2Block(cfg) for _ in range(cfg.n_layers)])\n",
                "        self.norm_f = nn.LayerNorm(cfg.d_model)\n",
                "        self.lm_head = nn.Linear(cfg.d_model, cfg.vocab_size, bias=False)\n",
                "        self.lm_head.weight = self.embedding.weight\n",
                "\n",
                "    def forward(self, x):\n",
                "        x = self.embedding(x)\n",
                "        for layer in self.layers:\n",
                "            x = x + layer(x)\n",
                "        x = self.norm_f(x)\n",
                "        return self.lm_head(x)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ==========================================\n",
                "# 4. Main Execution\n",
                "# ==========================================\n",
                "\n",
                "# 1. Load Data\n",
                "print(\"Loading Dataset (beomi/KoAlpaca-v1.1a)...\")\n",
                "ds_raw = load_dataset(\"beomi/KoAlpaca-v1.1a\", split=\"train\")\n",
                "\n",
                "# 2. Train/Load Tokenizer\n",
                "custom_tokenizer = train_custom_tokenizer(ds_raw, vocab_size=config.vocab_size)\n",
                "tokenizer = WrappedTokenizer(custom_tokenizer)\n",
                "print(f\"Tokenizer Ready. Vocab Size: {tokenizer.vocab_size}\")\n",
                "config.vocab_size = tokenizer.vocab_size\n",
                "\n",
                "# 3. Prepare DataLoader\n",
                "train_ds = FastKoDataset(ds_raw, tokenizer, config.seq_len)\n",
                "train_loader = DataLoader(train_ds, batch_size=config.batch_size, shuffle=True)\n",
                "\n",
                "# 4. Initialize Model\n",
                "model = NanoMamba2(config).to(config.device)\n",
                "print(f\"Model Params: {sum(p.numel() for p in model.parameters())/1e6:.2f}M\")\n",
                "\n",
                "# 5. Optimizers (Muon + AdamW)\n",
                "muon_params = []\n",
                "adam_params = []\n",
                "for name, p in model.named_parameters():\n",
                "    if p.ndim == 2 and p.size(0) > 32 and p.size(1) > 32:\n",
                "        muon_params.append(p)\n",
                "    else:\n",
                "        adam_params.append(p)\n",
                "        \n",
                "optim_muon = Muon(muon_params, lr=config.lr_muon)\n",
                "optim_adam = torch.optim.AdamW(adam_params, lr=config.lr_adam)\n",
                "print(f\"Optimizer: Muon ({len(muon_params)} params) + AdamW ({len(adam_params)} params)\")\n",
                "\n",
                "# 6. Training Loop\n",
                "model.train()\n",
                "print(\"Starting Training (loss should decrease from ~8.0 to ~4.0)...\")\n",
                "for epoch in range(config.epochs):\n",
                "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config.epochs}\")\n",
                "    for step, inputs in enumerate(pbar):\n",
                "        inputs = inputs.to(config.device)\n",
                "        input_ids = inputs[:, :-1]\n",
                "        target_ids = inputs[:, 1:]\n",
                "        \n",
                "        optim_muon.zero_grad()\n",
                "        optim_adam.zero_grad()\n",
                "        \n",
                "        logits = model(input_ids)\n",
                "        loss = F.cross_entropy(logits.reshape(-1, config.vocab_size), \n",
                "                               target_ids.reshape(-1), \n",
                "                               ignore_index=tokenizer.pad_token_id)\n",
                "        \n",
                "        # NaN Check\n",
                "        if torch.isnan(loss):\n",
                "            print(\"!! LOSS IS NAN !! Stopping.\")\n",
                "            break\n",
                "            \n",
                "        loss.backward()\n",
                "        \n",
                "        optim_muon.step()\n",
                "        optim_adam.step()\n",
                "        \n",
                "        pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
                "        \n",
                "        # Simple Generation Preview\n",
                "        if step % 100 == 0:\n",
                "            with torch.no_grad():\n",
                "                ctx = torch.tensor([tokenizer.encode(\"Q:대한민국의 수도는? A:\")]).to(config.device)\n",
                "                for _ in range(20):\n",
                "                    logits = model(ctx)\n",
                "                    next_token = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True)\n",
                "                    ctx = torch.cat([ctx, next_token], dim=1)\n",
                "                    if next_token.item() == tokenizer.eos_token_id: break\n",
                "                print(f\"\\n[Preview] {tokenizer.decode(ctx[0].tolist())}\\n\")\n",
                "\n",
                "print(\"Training Complete.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
