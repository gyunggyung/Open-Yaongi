{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸ± Yaongi-Next Tiny MoE-Teon Experiment (Colab Free Tier)\n",
                "\n",
                "ì´ ë…¸íŠ¸ë¶ì€ H100 ëŒ€ê·œëª¨ í•™ìŠµ ì „, **Nemotron-3 + Teon + MoE** ì•„í‚¤í…ì²˜ê°€ ì •ìƒì ìœ¼ë¡œ ìˆ˜ë ´í•˜ëŠ”ì§€ í™•ì¸í•˜ê¸° ìœ„í•œ **ì´ˆì†Œí˜• ê²€ì¦ìš©**ìž…ë‹ˆë‹¤.\n",
                "\n",
                "### ðŸŽ¯ íŠ¹ì§•\n",
                "1. **Single File**: ë³„ë„ì˜ íŒŒì´ì¬ ìŠ¤í¬ë¦½íŠ¸ ì—…ë¡œë“œ ì—†ì´ ì´ ë…¸íŠ¸ë¶ í•˜ë‚˜ë§Œ ì—´ë©´ ë©ë‹ˆë‹¤.\n",
                "2. **Pure PyTorch**: `mamba_ssm` ì»´íŒŒì¼ ì—†ì´ ìˆœìˆ˜ PyTorchë¡œ ë™ìž‘í•˜ì—¬ T4/CPU ì–´ë””ì„œë“  ì¦‰ì‹œ ì‹¤í–‰ë©ë‹ˆë‹¤.\n",
                "3. **Teon Optimizer**: ë…¼ë¬¸ì˜ 2D Orthogonalization ê¸°ë²•ì´ ì ìš©ë˜ì–´ ìžˆìŠµë‹ˆë‹¤."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
                "!pip install torch transformers einops scipy sentencepiece huggingface_hub\n",
                "# mamba-ssmì€ ì»´íŒŒì¼ì´ ì˜¤ëž˜ ê±¸ë ¤ ìƒëžµí•©ë‹ˆë‹¤ (Pure PyTorch Fallback ì‚¬ìš©)\n",
                "# try:\n",
                "#     !pip install mamba-ssm causal-conv1d\n",
                "# except:\n",
                "#     print(\"Mamba installation skipped, using Torch fallback\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3. ëª¨ë¸ ë° í•™ìŠµ ì½”ë“œ ì •ì˜\n",
                "import sys\n",
                "import os\n",
                "\n",
                "# --- ðŸ± Yaongi Fix: Transformers 'module __main__' error patch ---\n",
                "try:\n",
                "    if not hasattr(sys.modules['__main__'], '__file__'):\n",
                "        # Create dummy file to satisfy transformers inspection\n",
                "        _dummy = 'colab_kernel_dummy.py'\n",
                "        with open(_dummy, 'w') as f: f.write('# dummy')\n",
                "        sys.modules['__main__'].__file__ = os.path.abspath(_dummy)\n",
                "        print('âœ… Patched __main__.__file__ for transformers compatibility')\n",
                "except Exception as e:\n",
                "    print(f'âš ï¸ Patch failed: {e}')\n",
                "# ----------------------------------------------------------------\n",
                "\n",
                "import os\n",
                "import math\n",
                "import time\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "from torch.utils.data import DataLoader, IterableDataset\n",
                "from transformers import AutoTokenizer, PreTrainedModel, PretrainedConfig\n",
                "from huggingface_hub import upload_folder\n",
                "from tqdm import tqdm\n",
                "\n",
                "# --- Configuration ---\n",
                "CONFIG_DICT = {\n",
                "    \"model_type\": \"yaongi_nemotron_tiny\",\n",
                "    \"n_layers\": 8,           \n",
                "    \"d_model\": 256,          \n",
                "    \"d_state\": 64,           \n",
                "    \"vocab_size\": 32768,\n",
                "    \"num_experts\": 8,        \n",
                "    \"top_k\": 2,\n",
                "    \"n_heads\": 8,\n",
                "    \"n_kv_heads\": 2,\n",
                "    \"engram_layers\": [1],\n",
                "    \"engram_avg_pool\": 2,\n",
                "    \"init_scale\": 0.01,\n",
                "    \"layer_norm_epsilon\": 1e-5,\n",
                "    \"tie_word_embeddings\": False,\n",
                "}\n",
                "\n",
                "TRAIN_CONFIG = {\n",
                "    \"project_name\": \"Yaongi-Tiny-Colab-Test\",\n",
                "    \"batch_size\": 32,\n",
                "    \"grad_accum\": 1,\n",
                "    \"max_seq_len\": 512,\n",
                "    \"max_steps\": 200,\n",
                "    \"save_steps\": 50,\n",
                "    \"push_to_hub\": False,\n",
                "    \"hf_repo_id\": \"gyung/Yaongi-Tiny-Test\",\n",
                "    \"lr_max\": 2e-3,\n",
                "    \"lr_min\": 1e-4,\n",
                "    \"warmup_steps\": 20,\n",
                "    \"decay_start_step\": 160,\n",
                "}\n",
                "\n",
                "# --- Teon Optimizer ---\n",
                "def zeropower_via_newtonschulz5(G, steps=5):\n",
                "    assert len(G.shape) in (2, 3)\n",
                "    a, b, c = (3.4445, -4.7750,  2.0315)\n",
                "    X = G.float()\n",
                "    if G.ndim == 2:\n",
                "        if G.size(0) > G.size(1): X = X.T\n",
                "    else:\n",
                "        if G.size(1) > G.size(2): X = X.transpose(1, 2)\n",
                "    X = X / (X.norm(dim=-1, keepdim=True).norm(dim=-2, keepdim=True) + 1e-7)\n",
                "    for _ in range(steps):\n",
                "        if X.ndim == 2:\n",
                "            A = X @ X.T\n",
                "        else:\n",
                "            A = torch.bmm(X, X.transpose(1, 2))\n",
                "        B = b * A + c * (torch.bmm(A, A) if X.ndim==3 else A @ A)\n",
                "        if X.ndim == 2:\n",
                "            X = a * X + B @ X\n",
                "        else:\n",
                "            X = a * X + torch.bmm(B, X)\n",
                "    if G.ndim == 2:\n",
                "        if G.size(0) > G.size(1): X = X.T\n",
                "    else:\n",
                "        if G.size(1) > G.size(2): X = X.transpose(1, 2)\n",
                "    return X.to(G.dtype)\n",
                "\n",
                "class Teon(torch.optim.Optimizer):\n",
                "    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5):\n",
                "        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)\n",
                "        super().__init__(params, defaults)\n",
                "    @torch.no_grad()\n",
                "    def step(self):\n",
                "        for group in self.param_groups:\n",
                "            lr = group['lr']\n",
                "            momentum = group['momentum']\n",
                "            nesterov = group['nesterov']\n",
                "            ns_steps = group['ns_steps']\n",
                "            for p in group['params']:\n",
                "                if p.grad is None: continue\n",
                "                g = p.grad\n",
                "                state = self.state[p]\n",
                "                if 'momentum_buffer' not in state:\n",
                "                    state['momentum_buffer'] = torch.zeros_like(p)\n",
                "                buf = state['momentum_buffer']\n",
                "                buf.mul_(momentum).add_(g)\n",
                "                if nesterov:\n",
                "                    update = g.add(buf, alpha=momentum)\n",
                "                else:\n",
                "                    update = buf\n",
                "                if update.ndim >= 2:\n",
                "                    g_ortho = zeropower_via_newtonschulz5(update, steps=ns_steps)\n",
                "                    if update.ndim == 2:\n",
                "                        scale = max(1, p.size(0)/p.size(1))**0.5\n",
                "                    else:\n",
                "                        scale = max(1, p.size(1)/p.size(2))**0.5\n",
                "                    p.data.add_(g_ortho, alpha=-lr * scale)\n",
                "                else:\n",
                "                    p.data.add_(update, alpha=-lr)\n",
                "\n",
                "# --- Architecture ---\n",
                "class YaongiConfig(PretrainedConfig):\n",
                "    model_type = \"yaongi_nemotron\"\n",
                "    def __init__(self, **kwargs):\n",
                "        super().__init__(**kwargs)\n",
                "        for k, v in CONFIG_DICT.items():\n",
                "            setattr(self, k, kwargs.get(k, v))\n",
                "    \n",
                "    @property\n",
                "    def _can_set_experts_implementation(self):\n",
                "        return False\n",
                "\n",
                "class SquaredReLU(nn.Module):\n",
                "    def forward(self, x): return torch.relu(x).pow(2)\n",
                "\n",
                "class RMSNorm(nn.Module):\n",
                "    def __init__(self, d, eps=1e-6):\n",
                "        super().__init__()\n",
                "        self.weight = nn.Parameter(torch.ones(d))\n",
                "        self.eps = eps\n",
                "    def forward(self, x):\n",
                "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps) * self.weight\n",
                "\n",
                "class Mamba2Block(nn.Module):\n",
                "    def __init__(self, d_model, d_state=64):\n",
                "        super().__init__()\n",
                "        self.in_proj = nn.Linear(d_model, d_model*2 + d_state*2 + 8, bias=False)\n",
                "        self.out_proj = nn.Linear(d_model, d_model, bias=False)\n",
                "    def forward(self, x):\n",
                "        # Simple Simulation for CPU/T4 Test without Kernel\n",
                "        B, T, C = x.shape\n",
                "        h = self.in_proj(x)\n",
                "        h = F.silu(h[:, :, :C]) \n",
                "        return self.out_proj(h)\n",
                "\n",
                "class NemotronMoE(nn.Module):\n",
                "    def __init__(self, d_model, num_experts=8, top_k=2):\n",
                "        super().__init__()\n",
                "        self.num_experts = num_experts\n",
                "        self.top_k = top_k\n",
                "        self.router = nn.Linear(d_model, num_experts, bias=False) \n",
                "        self.experts_up = nn.ModuleList([nn.Linear(d_model, d_model * 4, bias=False) for _ in range(num_experts)])\n",
                "        self.experts_down = nn.ModuleList([nn.Linear(d_model * 4, d_model, bias=False) for _ in range(num_experts)])\n",
                "        self.activation = SquaredReLU()\n",
                "        self.shared_expert = nn.Sequential(\n",
                "             nn.Linear(d_model, d_model * 4, bias=False),\n",
                "             SquaredReLU(),\n",
                "             nn.Linear(d_model * 4, d_model, bias=False)\n",
                "        )\n",
                "    def forward(self, x):\n",
                "        B, T, D = x.shape\n",
                "        x_flat = x.view(-1, D)\n",
                "        logits = self.router(x_flat)\n",
                "        probs = torch.sigmoid(logits)\n",
                "        topk_probs, topk_indices = torch.topk(probs, self.top_k, dim=-1)\n",
                "        topk_probs = topk_probs / (topk_probs.sum(dim=-1, keepdim=True) + 1e-6)\n",
                "        out = self.shared_expert(x_flat) \n",
                "        for k in range(self.top_k):\n",
                "            expert_indices_k = topk_indices[:, k]\n",
                "            val_k = topk_probs[:, k].unsqueeze(-1)\n",
                "            for e in range(self.num_experts):\n",
                "                 mask = (expert_indices_k == e)\n",
                "                 if mask.any():\n",
                "                     sub_x = x_flat[mask]\n",
                "                     ex_out = self.experts_down[e](self.activation(self.experts_up[e](sub_x)))\n",
                "                     out[mask] += val_k[mask] * ex_out\n",
                "        return out.view(B, T, D)\n",
                "\n",
                "class SimpleEngram(nn.Module):\n",
                "    def __init__(self, d_model, vocab_size):\n",
                "        super().__init__()\n",
                "        self.gate = nn.Linear(d_model, d_model, bias=False)\n",
                "    def forward(self, x, input_ids):\n",
                "        return x + torch.sigmoid(self.gate(x))\n",
                "\n",
                "class GQAttention(nn.Module):\n",
                "    def __init__(self, d_model, n_heads=8, n_kv_heads=2):\n",
                "        super().__init__()\n",
                "        self.n_heads = n_heads\n",
                "        self.n_kv_heads = n_kv_heads\n",
                "        self.head_dim = d_model // n_heads\n",
                "        self.q_proj = nn.Linear(d_model, d_model, bias=False)\n",
                "        self.k_proj = nn.Linear(d_model, self.head_dim * n_kv_heads, bias=False)\n",
                "        self.v_proj = nn.Linear(d_model, self.head_dim * n_kv_heads, bias=False)\n",
                "        self.o_proj = nn.Linear(d_model, d_model, bias=False)\n",
                "    def forward(self, x):\n",
                "        B, T, C = x.shape\n",
                "        q = self.q_proj(x).view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
                "        k = self.k_proj(x).view(B, T, self.n_kv_heads, self.head_dim).transpose(1, 2)\n",
                "        v = self.v_proj(x).view(B, T, self.n_kv_heads, self.head_dim).transpose(1, 2)\n",
                "        k = k.repeat_interleave(self.n_heads // self.n_kv_heads, dim=1)\n",
                "        v = v.repeat_interleave(self.n_heads // self.n_kv_heads, dim=1)\n",
                "        out = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
                "        return self.o_proj(out.transpose(1, 2).reshape(B, T, C))\n",
                "\n",
                "class HybridMoEEngram(PreTrainedModel):\n",
                "    config_class = YaongiConfig\n",
                "    def __init__(self, config):\n",
                "        super().__init__(config)\n",
                "        self.config = config\n",
                "        d_model = config.d_model\n",
                "        self.embed = nn.Embedding(config.vocab_size, d_model)\n",
                "        self.layers = nn.ModuleList()\n",
                "        self.norms = nn.ModuleList()\n",
                "        for i in range(config.n_layers):\n",
                "            if i in config.engram_layers:\n",
                "                block = SimpleEngram(d_model, config.vocab_size)\n",
                "            elif i % 2 == 1:\n",
                "                block = GQAttention(d_model, config.n_heads, config.n_kv_heads)\n",
                "            else:\n",
                "                block = nn.Sequential(Mamba2Block(d_model, config.d_state), NemotronMoE(d_model, config.num_experts, config.top_k))\n",
                "            self.layers.append(block)\n",
                "            self.norms.append(RMSNorm(d_model))\n",
                "        self.final_norm = RMSNorm(d_model)\n",
                "        self.lm_head = nn.Linear(d_model, config.vocab_size, bias=False) \n",
                "        self.post_init()\n",
                "    def _init_weights(self, module):\n",
                "        if isinstance(module, nn.Linear):\n",
                "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
                "        elif isinstance(module, nn.Embedding):\n",
                "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
                "    def forward(self, input_ids, labels=None):\n",
                "        x = self.embed(input_ids)\n",
                "        for layer, norm in zip(self.layers, self.norms):\n",
                "            residual = x\n",
                "            x = norm(x)\n",
                "            if isinstance(layer, SimpleEngram):\n",
                "                out = layer(x, input_ids)\n",
                "            else:\n",
                "                out = layer(x)\n",
                "            x = residual + out\n",
                "        logits = self.lm_head(self.final_norm(x))\n",
                "        loss = None\n",
                "        if labels is not None:\n",
                "            shift_logits = logits[..., :-1, :].contiguous()\n",
                "            shift_labels = labels[..., 1:].contiguous()\n",
                "            loss = F.cross_entropy(shift_logits.view(-1, self.config.vocab_size), shift_labels.view(-1))\n",
                "        return {'logits': logits, 'loss': loss}\n",
                "\n",
                "def get_wsd_lr(step, cfg):\n",
                "    max_lr = cfg['lr_max']; min_lr = cfg['lr_min']\n",
                "    warmup = cfg['warmup_steps']; decay_start = cfg['decay_start_step']; max_steps = cfg['max_steps']\n",
                "    if step < warmup: return max_lr * (step / warmup)\n",
                "    if step < decay_start: return max_lr\n",
                "    decay_steps = max_steps - decay_start\n",
                "    progress = (step - decay_start) / decay_steps\n",
                "    return max_lr - (max_lr - min_lr) * progress"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4. ì‹¤í–‰ Loop\n",
                "def train():\n",
                "    cfg = TRAIN_CONFIG\n",
                "    \n",
                "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "    print(f\"Running on {device}\")\n",
                "    \n",
                "    config = YaongiConfig(**CONFIG_DICT)\n",
                "    model = HybridMoEEngram(config).to(device)\n",
                "    print(f\"Model Parameters: {sum(p.numel() for p in model.parameters())/1e6:.2f}M\")\n",
                "    \n",
                "    teon_params = [p for p in model.parameters() if p.ndim >= 2]\n",
                "    adam_params = [p for p in model.parameters() if p.ndim < 2]\n",
                "\n",
                "    optim_teon = Teon(teon_params, lr=cfg['lr_max'])\n",
                "    optim_adam = torch.optim.AdamW(adam_params, lr=0.001)\n",
                "    \n",
                "    class DummyDataset(IterableDataset):\n",
                "        def __init__(self, seq_len): self.seq_len = seq_len\n",
                "        def __iter__(self):\n",
                "            while True: yield torch.randint(0, 32000, (self.seq_len + 1,))\n",
                "            \n",
                "    loader = DataLoader(DummyDataset(cfg['max_seq_len']), batch_size=cfg['batch_size'])\n",
                "    model.train()\n",
                "    step = 0\n",
                "    pbar = tqdm(total=cfg['max_steps'])\n",
                "    \n",
                "    for batch in loader:\n",
                "        if step >= cfg['max_steps']: break\n",
                "        lr = get_wsd_lr(step, cfg)\n",
                "        for g in optim_teon.param_groups: g['lr'] = lr\n",
                "        outputs = model(batch.to(device), labels=batch.to(device))\n",
                "        loss = outputs['loss']\n",
                "        loss.backward()\n",
                "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
                "        optim_teon.step(); optim_adam.step()\n",
                "        optim_teon.zero_grad(); optim_adam.zero_grad()\n",
                "        step += 1\n",
                "        pbar.update(1)\n",
                "        pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
                "\n",
                "train()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}